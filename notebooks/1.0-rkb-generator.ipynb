{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes, out_planes, bias=False):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "def conv3x3(in_planes, out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nc = x.size(1)\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)\n",
    "        return x[:, :nc] * torch.sigmoid(x[:, nc:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef, train):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        if train:\n",
    "            self.nef = nef\n",
    "        else:\n",
    "            self.nef = 256  # define a uniform ranker, this is TEXT.embedding_dimension\n",
    "\n",
    "        model = models.inception_v3(init_weights = True)\n",
    "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "        model.load_state_dict(model_zoo.load_url(url))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('Load pretrained model from ', url)\n",
    "\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # this is the image size\n",
    "        # x.shape: 10 3 256 256\n",
    "\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "\n",
    "        # feature.shape: 10 256 17 17\n",
    "        # cnn_code.shape: 10 256\n",
    "        return features, cnn_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_enoder = CNN_ENCODER(256, True)\n",
    "x = torch.randn(10, 3, 256, 256)\n",
    "features, cnn_code = img_enoder(x)\n",
    "print(features.shape)\n",
    "print(cnn_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample / Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.InstanceNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "\n",
    "def imgUpBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=1.9, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.InstanceNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "def downBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num * 2),\n",
    "            nn.InstanceNorm2d(channel_num * 2),\n",
    "            GLU(),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.InstanceNorm2d(channel_num))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_block = ResBlock(3)\n",
    "x = torch.randn(10, 3, 256, 256)\n",
    "out = residual_block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA_NET(nn.Module):\n",
    "    # some code is modified from vae examples\n",
    "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.t_dim = cfg.TEXT.EMBEDDING_DIM # 256\n",
    "        self.c_dim = cfg.GAN.CONDITION_DIM # 100\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n",
    "        self.relu = GLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar, device):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std, device = device)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "\n",
    "class INIT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, ncf, nef):\n",
    "        super(INIT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.in_dim = cfg.GAN.Z_DIM + ncf + cfg.TEXT.EMBEDDING_DIM #GAN.Z_DIM = 100, TEXT.EMBEDDING_DIM = 256\n",
    "        self.ef_dim = nef\n",
    "\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        nz, ngf = self.in_dim, self.gf_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
    "            GLU())\n",
    "\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
    "        self.upsample4 = upBlock(ngf // 8 * 3, ngf // 16)\n",
    "\n",
    "        self.residual = self._make_layer(ResBlock, ngf // 8 * 3)\n",
    "        self.ACM = ACM(ngf // 8 * 3)\n",
    "\n",
    "        self.att = SPATIAL_NET(ngf // 8, self.ef_dim)\n",
    "        self.channel_att = CHANNEL_NET(ngf // 8, self.ef_dim, 32)\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM): #R_NUM = 2\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z_code, c_code, cnn_code, imgs, mask, word_embs):\n",
    "        \n",
    "        c_z_code = torch.cat((c_code, z_code), 1)\n",
    "\n",
    "        # for testing\n",
    "        if not cfg.TRAIN.FLAG and not cfg.B_VALIDATION:\n",
    "            cnn_code = cnn_code.repeat(c_z_code.size(0), 1)\n",
    "\n",
    "        c_z_cnn_code = torch.cat((c_z_code, cnn_code), 1)\n",
    "        out_code = self.fc(c_z_cnn_code)\n",
    "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
    "        out_code = self.upsample1(out_code)\n",
    "        out_code = self.upsample2(out_code)\n",
    "        out_code32 = self.upsample3(out_code)\n",
    "\n",
    "        self.att.applyMask(mask)\n",
    "        c_code, att = self.att(out_code32, word_embs)\n",
    "        c_code_channel, att_channel = self.channel_att(c_code, word_embs, out_code32.size(2), out_code32.size(3))\n",
    "        c_code = c_code.view(word_embs.size(0), -1, out_code32.size(2), out_code32.size(3))\n",
    "        h_c_code = torch.cat((out_code32, c_code), 1)\n",
    "        h_c_c_code = torch.cat((h_c_code, c_code_channel), 1)\n",
    "\n",
    "\n",
    "        out_imgs_code32 = self.ACM(h_c_c_code, imgs)\n",
    "        out_imgs_code32 = self.residual(out_imgs_code32)\n",
    "        out_code64 = self.upsample4(out_imgs_code32)\n",
    "        return out_code64\n",
    "\n",
    "class NEXT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, nef, ncf, size):\n",
    "        super(NEXT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.ef_dim = nef\n",
    "        self.cf_dim = ncf\n",
    "        self.num_residual = cfg.GAN.R_NUM #R_NUM = 2\n",
    "        self.size = size\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM): #R_NUM = 2\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        self.att = SPATIAL_NET(ngf, self.ef_dim)\n",
    "        self.channel_att = CHANNEL_NET(ngf, self.ef_dim, self.size)\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 3)\n",
    "        self.upsample = upBlock(ngf * 3, ngf)\n",
    "        self.ACM = ACM(ngf * 3)\n",
    "        self.upsample2 = upBlock(ngf, ngf)\n",
    "\n",
    "    def forward(self, h_code, c_code, word_embs, mask, seg_img):\n",
    "        \"\"\"\n",
    "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
    "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
    "            c_code1: batch x idf x queryL\n",
    "            att1: batch x sourceL x queryL\n",
    "        \"\"\"\n",
    "        self.att.applyMask(mask)\n",
    "        c_code, att = self.att(h_code, word_embs)\n",
    "        c_code_channel, att_channel = self.channel_att(c_code, word_embs, h_code.size(2), h_code.size(3))\n",
    "        c_code = c_code.view(word_embs.size(0), -1, h_code.size(2), h_code.size(3))\n",
    "\n",
    "        h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        h_c_c_code = torch.cat((h_c_code, c_code_channel), 1)\n",
    "        h_c_c_seg_code = self.ACM(h_c_c_code, seg_img)\n",
    "\n",
    "        out_code = self.residual(h_c_c_seg_code)\n",
    "\n",
    "        out_code = self.upsample(out_code)\n",
    "        out_code = self.upsample2(out_code)\n",
    "\n",
    "        return out_code, att\n",
    "\n",
    "class GET_IMAGE_G(nn.Module):\n",
    "    def __init__(self, ngf):\n",
    "        super(GET_IMAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, h_code):\n",
    "        out_img = self.img(h_code)\n",
    "        return out_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_NET, self).__init__()\n",
    "        ngf = cfg.GAN.GF_DIM #32\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM #256\n",
    "        ncf = cfg.GAN.CONDITION_DIM #100\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        if cfg.TREE.BRANCH_NUM > 0: #BRANCH_NUM for train_bird is set at 3\n",
    "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf, nef)\n",
    "            self.imgUpSample1 = imgUpBlock(nef, ngf)\n",
    "            \n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf, 64)\n",
    "            self.img_net = GET_IMAGE_G(ngf)\n",
    "            self.ACM = ACM(ngf)\n",
    "            self.imgUpSample2 = downBlock(nef//2, ngf)\n",
    "            self.imgUpSample3 = upBlock(ngf, ngf)\n",
    "            self.imgUpSample4 = upBlock(ngf, ngf)\n",
    "    def forward(self, z_code, sent_emb, word_embs, mask, cnn_code, region_features, vgg_features):\n",
    "        \n",
    "        fake_imgs = []\n",
    "        att_maps = []\n",
    "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            img_code32 = self.imgUpSample1(region_features)\n",
    "            h_code1 = self.h_net1(z_code, c_code, cnn_code, img_code32, mask, word_embs)\n",
    "\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            img_code64 = self.imgUpSample2(vgg_features)\n",
    "            h_code2, att2 = \\\n",
    "                self.h_net3(h_code1, c_code, word_embs, mask, img_code64)\n",
    "            img_code128 = self.imgUpSample3(img_code64)            \n",
    "            img_code256 = self.imgUpSample4(img_code128)            \n",
    "            h_code3 = self.ACM(h_code2, img_code256)\n",
    "            fake_img = self.img_net(h_code3)\n",
    "            fake_imgs.append(fake_img)\n",
    "            if att2 is not None:\n",
    "                att_maps.append(att2)\n",
    "        \n",
    "        return fake_imgs, att_maps, mu, logvar\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc37b074dfff84c4ccd952270a7ae60ba709a7115c3f4beb9b7542b0dd613dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
