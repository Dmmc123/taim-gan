{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "from typing import Any\n",
    "\n",
    "from .conv_utils import conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes, out_planes, bias=False):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "def conv3x3(in_planes, out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: int = 3,\n",
    "    stride: int = 1,\n",
    "    padding: int = 1,\n",
    ") -> nn.Conv2d:\n",
    "    \"\"\"\n",
    "    Template convolution which is typically used throughout the project\n",
    "\n",
    "    :param int in_channels: Number of input channels\n",
    "    :param int out_channels: Number of output channels\n",
    "    :param int kernel_size: Size of sliding kernel\n",
    "    :param int stride: How many steps kernel does when sliding\n",
    "    :param int padding: How many dimensions to pad\n",
    "    :return: Convolution layer with parameters\n",
    "    :rtype: nn.Conv2d\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "    )\n",
    "\n",
    "def conv1d(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> nn.Conv1d:\n",
    "    \"\"\"\n",
    "    Template 1d convolution which is typically used throughout the project\n",
    "\n",
    "    :param int in_channels: Number of input channels\n",
    "    :param int out_channels: Number of output channels\n",
    "    :param int kernel_size: Size of sliding kernel\n",
    "    :param int stride: How many steps kernel does when sliding\n",
    "    :param int padding: How many dimensions to pad\n",
    "    :return: Convolution layer with parameters\n",
    "    :rtype: nn.Conv2d\n",
    "    \"\"\"\n",
    "    return nn.Conv1d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "    )\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nc = x.size(1)\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)\n",
    "        return x[:, :nc] * torch.sigmoid(x[:, nc:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACM(nn.Module):\n",
    "    \"\"\"Affine Combination Module from ManiGAN\"\"\"\n",
    "\n",
    "    def __init__(self, text_chans: int, img_chans: int, inner_dim: int = 64) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the convolutional layers\n",
    "\n",
    "        :param int text_chans: Channels of textual input\n",
    "        :param int img_chans: Channels in visual input\n",
    "        :param int inner_dim: Hyperparameters for inner dimensionality of features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = conv2d(in_channels=img_chans, out_channels=inner_dim)\n",
    "        self.weights = conv2d(in_channels=inner_dim, out_channels=text_chans)\n",
    "        self.biases = conv2d(in_channels=inner_dim, out_channels=text_chans)\n",
    "\n",
    "    def forward(self, text: torch.Tensor, img: torch.Tensor) -> Any:\n",
    "        \"\"\"\n",
    "        Propagate the textual and visual input through the ACM module\n",
    "\n",
    "        :param torch.Tensor text: Textual input\n",
    "        :param torch.Tensor img: Image input\n",
    "        :return: Affine combination of text and image\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        img_features = self.conv(img)\n",
    "        return text * self.weights(img_features) + self.biases(img_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention module for attending textual context to visual features\"\"\"\n",
    "\n",
    "    def __init__(self, d: int, d_hat: int) -> None:\n",
    "        \"\"\"\n",
    "        Set up softmax and conv layers\n",
    "\n",
    "        :param int d: Initial embedding size for textual features. D from paper\n",
    "        :param int d_hat: Height of image feature map. D_hat from paper\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(2)\n",
    "        self.conv = conv1d(d, d_hat)\n",
    "\n",
    "    def forward(self, text_context: torch.Tensor, image: torch.Tensor) -> Any:\n",
    "        \"\"\"\n",
    "        Project image features into the latent space\n",
    "        of textual features and apply attention\n",
    "\n",
    "        :param text_context: D x T tensor of hidden textual features\n",
    "        :param image: D_hat x N visual features\n",
    "        :return: Word features attended by visual features\n",
    "        :rtype: Any\n",
    "        \"\"\"\n",
    "        text_context = self.conv(text_context)\n",
    "        image = torch.transpose(image, 1, 2)\n",
    "        s_i_j = image @ text_context\n",
    "        b_i_j = self.softmax(s_i_j)\n",
    "        c_i_j = b_i_j @ torch.transpose(text_context, 1, 2)\n",
    "        return torch.transpose(c_i_j, 1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelWiseAttention(nn.Module):\n",
    "    \"\"\"ChannelWise attention adapted from ControlGAN\"\"\"\n",
    "\n",
    "    def __init__(self, fm_size: int, text_d: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Channel-Wise attention module\n",
    "\n",
    "        :param int fm_size:\n",
    "            Height and width of feature map on k-th iteration of forward-pass.\n",
    "            In paper, it's H_k * W_k\n",
    "        :param int text_d: Dimensionality of sentence. From paper, it's D\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # perception layer\n",
    "        self.text_conv = conv1d(text_d, fm_size)\n",
    "        # attention across channel dimension\n",
    "        self.softmax = nn.Softmax(2)\n",
    "\n",
    "    def forward(self, v_k: torch.Tensor, w_text: torch.Tensor) -> Any:\n",
    "        \"\"\"\n",
    "        Apply attention to visual features taking into account features of words\n",
    "\n",
    "        :param torch.Tensor v_k: Visual context\n",
    "        :param torch.Tensor w_text: Textual features\n",
    "        :return: Fused hidden visual features and word features\n",
    "        :rtype: Any\n",
    "        \"\"\"\n",
    "        w_hat = self.text_conv(w_text)\n",
    "        m_k = v_k @ w_hat\n",
    "        a_k = self.softmax(m_k)\n",
    "        w_hat = torch.transpose(w_hat, 1, 2)\n",
    "        return a_k @ w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef, train):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        if train:\n",
    "            self.nef = nef\n",
    "        else:\n",
    "            self.nef = 256  # define a uniform ranker, this is TEXT.embedding_dimension\n",
    "\n",
    "        model = models.inception_v3(init_weights = True)\n",
    "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "        model.load_state_dict(model_zoo.load_url(url))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('Load pretrained model from ', url)\n",
    "\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # this is the image size\n",
    "        # x.shape: 10 3 256 256\n",
    "\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "\n",
    "        # feature.shape: 10 256 17 17\n",
    "        # cnn_code.shape: 10 256\n",
    "        return features, cnn_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_enoder = CNN_ENCODER(256, True)\n",
    "x = torch.randn(10, 3, 256, 256)\n",
    "features, cnn_code = img_enoder(x)\n",
    "print(features.shape)\n",
    "print(cnn_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample / Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.InstanceNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "\n",
    "def imgUpBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=1.9, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.InstanceNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "def downBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num * 2),\n",
    "            nn.InstanceNorm2d(channel_num * 2),\n",
    "            GLU(),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.InstanceNorm2d(channel_num))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_block = ResBlock(3)\n",
    "x = torch.randn(10, 3, 256, 256)\n",
    "out = residual_block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'TEXT': {\n",
    "        'EMBEDDING_DIM': 256,\n",
    "        'CAPTIONS_PER_IMAGE': 10,\n",
    "        'WORDS_NUM': 18\n",
    "    },\n",
    "    'GAN': {\n",
    "        'DF_DIM': 64,\n",
    "        'GF_DIM': 32,\n",
    "        'Z_DIM': 100,\n",
    "        'R_NUM': 2,\n",
    "        'CONDITION_DIM': 100,\n",
    "        'B_ATTENTION': True,\n",
    "        'B_DCGAN': False,\n",
    "    },\n",
    "    'TRAIN': {\n",
    "        'FLAG': True,\n",
    "        'NET_G': '',\n",
    "        'NET_E': '',\n",
    "        'NET_C': '',\n",
    "        'NET_D': '',\n",
    "        'B_NET_D': True,\n",
    "        'BATCH_SIZE': 40,\n",
    "        'MAX_EPOCH': 600,\n",
    "        'SNAPSHOT_INTERVAL': 5,\n",
    "        'DISCRIMINATOR_LR': 0.0002,\n",
    "        'GENERATOR_LR': 0.0002,\n",
    "        'ENCODER_LR': 0.0002,\n",
    "        'RNN_GRAD_CLIP': 0.25,\n",
    "        'NET_E': '../models/DAMSMencoders/bird/text_encoder.pth', #TODO\n",
    "        'SMOOTH': {\n",
    "            'GAMMA1': 4.0,\n",
    "            'GAMMA2': 5.0,\n",
    "            'GAMMA3': 10.0,\n",
    "            'LAMBDA': 5.0\n",
    "        }\n",
    "    },\n",
    "    'TREE': {\n",
    "        'BRANCH_NUM': 3,\n",
    "        'BASE_SIZE': 64\n",
    "    },\n",
    "    'DATASET_NAME': 'birds',\n",
    "    'DATA_DIR': '../data/birds', #TODO\n",
    "    'CUDA': True,\n",
    "    'RNN_TYPE': 'LSTM',\n",
    "    'B_VALIDATION': False,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA_NET(nn.Module):\n",
    "    # some code is modified from vae examples\n",
    "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.t_dim = cfg.TEXT.EMBEDDING_DIM # 256\n",
    "        self.c_dim = cfg.GAN.CONDITION_DIM # 100\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n",
    "        self.relu = GLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar, device):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std, device = device)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "\n",
    "class INIT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, ncf, nef):\n",
    "        super(INIT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf #512, as we pass ngf * 16 to this class from G_NET.\n",
    "        self.in_dim = cfg.GAN.Z_DIM + ncf + cfg.TEXT.EMBEDDING_DIM #GAN.Z_DIM = 100, TEXT.EMBEDDING_DIM = 256, ncf = 100\n",
    "        self.ef_dim = nef #256, i.e. text.embedding_dim\n",
    "\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        nz, ngf = self.in_dim, self.gf_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
    "            GLU())\n",
    "\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8) # inputs are: 512 // 4 = 128, 512 // 8 = 64\n",
    "        self.upsample4 = upBlock(ngf // 8 * 3, ngf // 16)\n",
    "\n",
    "        self.residual = self._make_layer(ResBlock, ngf // 8 * 3)\n",
    "        self.ACM = ACM(ngf // 8 * 3, img_chans = cfg.GAN.GF_DIM) \n",
    "\n",
    "        self.att = SpatialAttention(self.ef_dim, ngf // 8) #passing 256, 64 to SpatialAttention.\n",
    "        self.channel_att = ChannelWiseAttention(32*32, self.ef_dim) #passing 32*32, 256 to ChannelWiseAttention\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM): #R_NUM = 2\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z_code, c_code, cnn_code, imgs, mask, word_embs):\n",
    "        \n",
    "        c_z_code = torch.cat((c_code, z_code), 1)\n",
    "\n",
    "        # for testing\n",
    "        if not cfg.TRAIN.FLAG and not cfg.B_VALIDATION:\n",
    "            cnn_code = cnn_code.repeat(c_z_code.size(0), 1)\n",
    "\n",
    "        c_z_cnn_code = torch.cat((c_z_code, cnn_code), 1)\n",
    "        \n",
    "        out_code = self.fc(c_z_cnn_code)\n",
    "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
    "        out_code = self.upsample1(out_code)\n",
    "        out_code = self.upsample2(out_code)\n",
    "        out_code32 = self.upsample3(out_code) #out_code32.shape = torch.Size([batch, channel, H, W])\n",
    "        out_code32_comb = out_code32.view(out_code32.shape[0], -1, out_code32.shape[2] * out_code32.shape[3])\n",
    "\n",
    "        # self.att.applyMask(mask)\n",
    "        c_code = self.att(word_embs, out_code32_comb) #c_code shape: D^ x N, words_embs shape: D x T, out_code32_comb shape: D^ x N\n",
    "        c_code_channel = self.channel_att(c_code, word_embs) #c_code_channel shape: D^ x N or C * (Hk * Wk)\n",
    "        c_code = c_code.view(word_embs.size(0), -1, out_code32.size(2), out_code32.size(3)) #shape is (batch, channel, H, W)\n",
    "        h_c_code = torch.cat((out_code32, c_code), 1)\n",
    "        c_code_channel = c_code_channel.view(word_embs.size(0), -1, out_code32.size(2), out_code32.size(3)) #shape is (batch, channel, H, W)\n",
    "        h_c_c_code = torch.cat((h_c_code, c_code_channel), 1)\n",
    "\n",
    "\n",
    "        out_imgs_code32 = self.ACM(h_c_c_code, imgs)\n",
    "        out_imgs_code32 = self.residual(out_imgs_code32)\n",
    "        out_code64 = self.upsample4(out_imgs_code32)\n",
    "        return out_code64\n",
    "\n",
    "class NEXT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, nef, ncf, size):\n",
    "        super(NEXT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf #32\n",
    "        self.ef_dim = nef\n",
    "        self.cf_dim = ncf\n",
    "        self.num_residual = cfg.GAN.R_NUM #R_NUM = 2\n",
    "        self.size = size # 64\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM): #R_NUM = 2\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        self.att = SpatialAttention(self.ef_dim, ngf) # passing 256, 32 to SpatialAttention.\n",
    "        self.channel_att = ChannelWiseAttention(self.size * self.size, self.ef_dim) #passing 64 * 64, 256 to ChannelWiseAttention\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 3)\n",
    "        self.upsample = upBlock(ngf * 3, ngf)\n",
    "        self.ACM = ACM(ngf * 3, img_chans = cfg.GAN.GF_DIM) #passing 96, 32 to ACM.\n",
    "        self.upsample2 = upBlock(ngf, ngf)\n",
    "\n",
    "    def forward(self, h_code, c_code, word_embs, mask, seg_img):\n",
    "        \"\"\"\n",
    "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
    "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
    "            c_code1: batch x idf x queryL\n",
    "            att1: batch x sourceL x queryL\n",
    "        \"\"\"\n",
    "        # self.att.applyMask(mask)\n",
    "        h_code_comb = h_code.view(h_code.shape[0], -1, h_code.shape[2] * h_code.shape[3])\n",
    "        c_code = self.att(word_embs, h_code_comb) #c_code shape: D^ x N, words_embs shape: D x T, h_code_comb shape: D^ x N \n",
    "        c_code_channel = self.channel_att(c_code, word_embs) #c_code_channel shape: D^ x N or C * (Hk * Wk)\n",
    "        c_code = c_code.view(word_embs.size(0), -1, h_code.size(2), h_code.size(3)) # shape is (batch, channel, H, W)\n",
    "\n",
    "        h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        c_code_channel = c_code_channel.view(word_embs.size(0), -1, h_code.size(2), h_code.size(3)) #shape is (batch, channel, H, W)\n",
    "        h_c_c_code = torch.cat((h_c_code, c_code_channel), 1)\n",
    "        h_c_c_seg_code = self.ACM(h_c_c_code, seg_img)\n",
    "\n",
    "        out_code = self.residual(h_c_c_seg_code)\n",
    "\n",
    "        out_code = self.upsample(out_code)\n",
    "        out_code = self.upsample2(out_code)\n",
    "\n",
    "        return out_code\n",
    "\n",
    "class GET_IMAGE_G(nn.Module):\n",
    "    def __init__(self, ngf):\n",
    "        super(GET_IMAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, h_code):\n",
    "        out_img = self.img(h_code)\n",
    "        return out_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_NET, self).__init__()\n",
    "        ngf = cfg.GAN.GF_DIM #32\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM #256\n",
    "        ncf = cfg.GAN.CONDITION_DIM #100\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        if cfg.TREE.BRANCH_NUM > 0: #BRANCH_NUM for train_bird is set at 3\n",
    "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf, nef)\n",
    "            self.imgUpSample1 = imgUpBlock(nef, ngf)\n",
    "            \n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf, 64)\n",
    "            self.img_net = GET_IMAGE_G(ngf)\n",
    "            self.ACM = ACM(ngf, img_chans = cfg.GAN.GF_DIM)\n",
    "            self.imgUpSample2 = downBlock(nef//2, ngf)\n",
    "            self.imgUpSample3 = upBlock(ngf, ngf)\n",
    "            self.imgUpSample4 = upBlock(ngf, ngf)\n",
    "    def forward(self, z_code, sent_emb, word_embs, mask, cnn_code, region_features, vgg_features):\n",
    "        \"\"\"\n",
    "            :param z_code: batch x cfg.GAN.Z_DIM\n",
    "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
    "            :param word_embs: batch x cdf x seq_len\n",
    "            :param mask: batch x seq_len\n",
    "            :return:\n",
    "        \"\"\"\n",
    "        fake_imgs = []\n",
    "        att_maps = []\n",
    "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            img_code32 = self.imgUpSample1(region_features)\n",
    "            h_code1 = self.h_net1(z_code, c_code, cnn_code, img_code32, mask, word_embs)\n",
    "\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            img_code64 = self.imgUpSample2(vgg_features)\n",
    "            h_code2, att2 = \\\n",
    "                self.h_net3(h_code1, c_code, word_embs, mask, img_code64)\n",
    "            img_code128 = self.imgUpSample3(img_code64)            \n",
    "            img_code256 = self.imgUpSample4(img_code128)            \n",
    "            h_code3 = self.ACM(h_code2, img_code256)\n",
    "            fake_img = self.img_net(h_code3)\n",
    "            fake_imgs.append(fake_img)\n",
    "            if att2 is not None:\n",
    "                att_maps.append(att2)\n",
    "        \n",
    "        return fake_imgs, att_maps, mu, logvar\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc37b074dfff84c4ccd952270a7ae60ba709a7115c3f4beb9b7542b0dd613dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
